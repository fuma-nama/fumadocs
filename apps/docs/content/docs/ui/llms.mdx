---
title: AI & LLMs
description: Integrate AI functionality to Fumadocs.
---

## Docs for LLM

You can make your docs site more AI-friendly with dedicated docs content for large language models.

To begin, make a `getLLMText` function that converts pages into static MDX content.
This is an example for Fumadocs MDX:

<include meta='title="lib/get-llm-text.ts"'>./get-llm-text.ts</include>

It requires `includeProcessedMarkdown` to be enabled:

```ts title="source.config.ts"
import { defineDocs } from 'fumadocs-mdx/config';

export const docs = defineDocs({
  docs: {
    // [!code ++:3]
    postprocess: {
      includeProcessedMarkdown: true,
    },
  },
});
```

### `llms-full.txt`

A version of docs for AIs to read.

```ts tab="Next.js" title="app/llms-full.txt/route.ts"
import { source } from '@/lib/source';
import { getLLMText } from '@/lib/get-llm-text';

// cached forever
export const revalidate = false;

export async function GET() {
  const scan = source.getPages().map(getLLMText);
  const scanned = await Promise.all(scan);

  return new Response(scanned.join('\n\n'));
}
```

```ts tab="React Router" title="app/routes/llms-full.ts"
// make sure to include this route in `routes.ts` & pre-rendering!
import { source } from '@/lib/source';
import { getLLMText } from '@/lib/get-llm-text';

export async function loader() {
  const scan = source.getPages().map(getLLMText);
  const scanned = await Promise.all(scan);

  return new Response(scanned.join('\n\n'));
}
```

```ts tab="Tanstack Start" title="src/routes/llms-full[.]txt.ts"
import { createServerFileRoute } from '@tanstack/react-start/server';
import { source } from '@/lib/source';
import { getLLMText } from '@/lib/get-llm-text';

export const ServerRoute = createServerFileRoute('/llms-full.txt').methods({
  GET: async () => {
    const scan = source.getPages().map(getLLMText);
    const scanned = await Promise.all(scan);
    return new Response(scanned.join('\n\n'));
  },
});
```

### `*.mdx` [#mdx-extension]

Allow people to append `.mdx` to a page to get its Markdown/MDX content.

On Next.js, you can make a route handler to return page content, and redirect users to that route.

<include meta='tab="app/llms.mdx/[[...slug]]/route.ts"'>./llms.mdx.ts</include>

```ts tab="next.config.ts"
import type { NextConfig } from 'next';

const config: NextConfig = {
  async rewrites() {
    return [
      {
        source: '/docs/:path*.mdx',
        destination: '/llms.mdx/:path*',
      },
    ];
  },
};
```

### Page Actions

Common page actions for AI, require [`*.mdx`](#mdx-extension) to be implemented first.

![AI Page Actions](/docs/ai-page-actions.png)

```npm
npx @fumadocs/cli add ai/page-actions
```

Use it in your docs page like:

```tsx title="app/docs/[[...slug]]/page.tsx"
<div className="flex flex-row gap-2 items-center border-b pt-2 pb-6">
  <LLMCopyButton markdownUrl={`${page.url}.mdx`} />
  <ViewOptions
    markdownUrl={`${page.url}.mdx`}
    githubUrl={`https://github.com/${owner}/${repo}/blob/dev/apps/docs/content/docs/${page.path}`}
  />
</div>
```

## Ask AI

![AI Search](/docs/ai-search.png)

You can install the AI search dialog using Fumadocs CLI:

```package-install
npx @fumadocs/cli add ai/search
```

You can add the trigger component to your root layout.

### AI Model

By default, it's configured for Inkeep AI using Vercel AI SDK. Update the configurations in `useChat` and `/api/chat` route to connect to your own AI model instead.

Note that Fumadocs doesn't provide the AI model, it's up to you.
Your AI model can use the `llms-full.txt` file generated above, or more diversified sources of information when combined with 3rd party solutions.
